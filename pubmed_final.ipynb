{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1ed89b-40a8-49da-8112-c2d52b2f123b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records for query 'machine learning AND venous thrombosis AND (1980[PDAT] : 2023[PDAT])': 84\n",
      "Processing batch 1/1 with PMIDs: 37370233, 37365805, 37348318, 37250843, 37089113, 37079979, 37043409, 36942630, 36912139, 36877716, 36750656, 36632097, 36596268, 36587511, 36580997, 36276886, 36274391, 36272528, 36220884, 36186967, 36169966, 36157936, 36072822, 36061353, 35932395, 35806137, 35791841, 35697739, 35648280, 35504312, 35414086, 35299099, 35272558, 35253466, 35204365, 35116072, 35055429, 35047634, 34950733, 34945749, 34934034, 34838025, 34773490, 34697635, 34581632, 34528949, 34492064, 34428931, 34344669, 34145330, 34137837, 34116215, 34107539, 34036817, 34003964, 33971352, 33928796, 33777638, 33625875, 33529319, 33431375, 33091585, 32920367, 32919186, 32780732, 32645000, 32353052, 32221349, 32110753, 31888439, 31697697, 31563130, 31562113, 31445252, 31374513, 31352086, 30727024, 29224926, 28592811, 28475051, 27885969, 26898369, 25332356, 24111448\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     paper_details\u001b[38;5;241m.\u001b[39mappend([query, title, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(authors), year, doi, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tags), abstract])\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Delay before the next API request\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Break the retry loop if request succeeds\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mIncompleteRead \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# Handle the IncompleteRead error\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import http.client\n",
    "import time\n",
    "from Bio import Entrez\n",
    "\n",
    "# Enter your email address (required by NCBI)\n",
    "Entrez.email = \"ddimopoulos@aegean.gr\"\n",
    "\n",
    "# Define your search terms\n",
    "keyword_list_1 = ['machine learning', 'artificial intelligence', 'deep learning', 'reinforcement learning', 'neural network']\n",
    "keyword_list_2 = ['venous thrombosis', 'venous thromboembolism', 'pulmonary embolism', 'deep vein thrombosis']\n",
    "\n",
    "# Define the publication years\n",
    "start_year = \"1980\"\n",
    "end_year = \"2023\"\n",
    "\n",
    "# Define the number of records to retrieve per batch (page)\n",
    "batch_size = 100\n",
    "\n",
    "# Define the number of retries for each PMID\n",
    "num_retries = 3\n",
    "\n",
    "# Define the delay between API requests (in seconds)\n",
    "delay = 1\n",
    "\n",
    "# Create a list to store the paper details\n",
    "paper_details = []\n",
    "\n",
    "# Search and retrieve paper details for each combination\n",
    "for keyword_1 in keyword_list_1:\n",
    "    for keyword_2 in keyword_list_2:\n",
    "        # Combine the two keywords with AND to retrieve papers that match both\n",
    "        query = f'{keyword_1} AND {keyword_2}'\n",
    "\n",
    "        # Append the publication years to the query\n",
    "        query += f' AND ({start_year}[PDAT] : {end_year}[PDAT])'\n",
    "\n",
    "        # Search in PubMed using the query\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=batch_size)\n",
    "        record = Entrez.read(handle)\n",
    "\n",
    "        # Get the total number of records for the query\n",
    "        total_records = int(record[\"Count\"])\n",
    "\n",
    "        # Calculate the number of batches (pages) required, limiting it to 30\n",
    "        num_batches = min((total_records - 1) // batch_size + 1, 30)\n",
    "\n",
    "        print(f\"Total records for query '{query}': {total_records}\")\n",
    "\n",
    "        # Retrieve paper details for each batch (page)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "\n",
    "            # Search in PubMed and retrieve the batch of records\n",
    "            handle = Entrez.esearch(db=\"pubmed\", term=query, retstart=start, retmax=batch_size, retmode=\"text\")\n",
    "            record = Entrez.read(handle)\n",
    "\n",
    "            # Get a list of PubMed IDs (PMIDs) for the retrieved papers\n",
    "            pmid_list = record[\"IdList\"]\n",
    "\n",
    "            print(f\"Processing batch {batch+1}/{num_batches} with PMIDs: {', '.join(pmid_list)}\")\n",
    "\n",
    "            # Retrieve the paper details for each PMID\n",
    "            for pmid in pmid_list:\n",
    "                retries = 0\n",
    "                while retries < num_retries:\n",
    "                    try:\n",
    "                        # Fetch the PubMed record for a specific PMID\n",
    "                        record_handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"medline\", retmode=\"text\")\n",
    "                        paper_record = record_handle.read()\n",
    "\n",
    "                        # Extract relevant information from the paper record\n",
    "                        title = \"\"\n",
    "                        authors = []\n",
    "                        year = \"\"\n",
    "                        doi = \"\"\n",
    "                        tags = []\n",
    "                        abstract = \"\"\n",
    "\n",
    "                        is_title = False\n",
    "                        is_tags = False\n",
    "                        is_abstract = False\n",
    "\n",
    "                        lines = paper_record.split('\\n')\n",
    "                        for line in lines:\n",
    "                            if line.startswith(\"TI  - \"):\n",
    "                                title = line.lstrip(\"TI  - \")\n",
    "                                is_title = True\n",
    "                            elif is_title and line.startswith(\" \"):\n",
    "                                title += line.strip()\n",
    "                            elif is_title:\n",
    "                                is_title = False\n",
    "                            elif line.startswith(\"AU  - \"):\n",
    "                                authors.append(line.lstrip(\"AU  - \"))\n",
    "                            elif line.startswith(\"DP  - \"):\n",
    "                                year = line.lstrip(\"DP  - \")\n",
    "                            elif line.startswith(\"LID - \"):\n",
    "                                doi = line.lstrip(\"LID - \")\n",
    "                            elif line.startswith(\"OT  - \"):\n",
    "                                tags.append(line.lstrip(\"OT  - \"))\n",
    "                            elif line.startswith(\"AB  - \"):\n",
    "                                abstract = line.lstrip(\"AB  - \")\n",
    "                                is_abstract = True\n",
    "                            elif is_abstract and line.startswith(\" \"):\n",
    "                                abstract += line.strip()\n",
    "                            elif is_abstract:\n",
    "                                is_abstract = False\n",
    "\n",
    "                        # Append the paper details to the list\n",
    "                        paper_details.append([query, title, ', '.join(authors), year, doi, ', '.join(tags), abstract])\n",
    "\n",
    "                        # Delay before the next API request\n",
    "                        time.sleep(delay)\n",
    "\n",
    "                        break  # Break the retry loop if request succeeds\n",
    "\n",
    "                    except http.client.IncompleteRead as e:\n",
    "                        # Handle the IncompleteRead error\n",
    "                        print(\"IncompleteRead error occurred:\", str(e))\n",
    "                        print(\"Retrying request for PMID:\", pmid)\n",
    "                        retries += 1\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file = \"results_PubMed_VerII.csv\"\n",
    "\n",
    "# Write the paper details to the CSV file\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Query\", \"Title\", \"Authors\", \"Year\", \"DOI\", \"Tags\", \"Abstract\"])  # Write the header\n",
    "    writer.writerows(paper_details)  # Write the rows\n",
    "\n",
    "print(\"Paper details extracted and saved to\", csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
